{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b9187a3",
   "metadata": {},
   "source": [
    "# Document AI OCR (sync)\n",
    "\n",
    "This notebook shows you how to do OCR on documents using the Google Cloud DocumentAI API synchronously. For the synchronous request the document content will be send as bytes and the program will block until it receives the response. The response is then visualized showing the preprocessed (e.g. rotated) image together with bounding boxes for block, paragraph, line and token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1354d2",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2049fa6e",
   "metadata": {},
   "source": [
    "### Set your Processor Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83bbbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"ffeldhaus-docai\"\n",
    "LOCATION = \"eu\"  # Format is 'us' or 'eu'\n",
    "\n",
    "# Sample invoices are stored in gs://cloud-samples-data/documentai/async_invoices/\n",
    "GCS_INPUT_BUCKET = 'cloud-samples-data'\n",
    "GCS_INPUT_PREFIX = 'documentai'\n",
    "\n",
    "LOCAL_INPUT_PATH = '../resources/general'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6c6fc",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary Python libraries and restart your kernel after.\n",
    "!pip install --quiet -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3f0e2",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d642f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../docai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117a4c9",
   "metadata": {},
   "source": [
    "## OCR Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc18367",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a990f25",
   "metadata": {},
   "source": [
    "Setup a new OCR processor by using `create_processor()`. To create the processor a documentai client needs to be initialized using `get_client`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb34fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s get_client,create_processor ../docai\n",
    "def create_processor(project_id: str, location: str, type_: str, display_name: str, kms_key_name: str = \"\"):\n",
    "    client = get_client(location = location)\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "    processor = documentai.Processor(\n",
    "        type_ = type_,\n",
    "        display_name = display_name,\n",
    "        kms_key_name = kms_key_name\n",
    "    )\n",
    "    result = client.create_processor(parent = parent, processor = processor)\n",
    "    print(f\"Processor {result.name} successfully created\")\n",
    "    return result\n",
    "\n",
    "def get_client(location: str):\n",
    "    client_options = ClientOptions(api_endpoint = f\"{location}-documentai.googleapis.com\")\n",
    "    return documentai.DocumentProcessorServiceClient(client_options = client_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0ba96",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = create_processor(\n",
    "    project_id = PROJECT_ID,\n",
    "    location = LOCATION,\n",
    "    type_ = \"OCR_PROCESSOR\",\n",
    "    display_name = \"OCR Processor\"\n",
    ")\n",
    "\n",
    "PROCESSOR_ID = processor.name.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c4222",
   "metadata": {},
   "source": [
    "### Process documents synchronously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6cc9a",
   "metadata": {},
   "source": [
    "Several helper functions have been created to simplify processing of documents. The content of a document, together with its MIME type, can be used in `process()` to process a document. This function uses `get_processor` to get an existing processor and then retrieves the processor details using `get_processor_details()` to check processor limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d63dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s list_processors,get_processor,fetch_processor_types,ProcessorLimits,ProcessorDetails,get_processor_details,process ../docai\n",
    "def list_processors(project_id: str, location: str):\n",
    "    client = get_client(location = location)\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "    return client.list_processors(parent = parent)\n",
    "\n",
    "def get_processor(project_id: str, location: str, processor_id: str):\n",
    "    processors = list_processors(project_id = project_id, location = location)\n",
    "    return next((processor for processor in processors  if f\"processors/{processor_id}\" in processor.name), None)\n",
    "\n",
    "def fetch_processor_types(project_id: str, location: str):\n",
    "    client = get_client(location = location)\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "    processor_types_response = client.fetch_processor_types(parent = parent)\n",
    "    return processor_types_response.processor_types\n",
    "\n",
    "class ProcessorLimits():\n",
    "    def __init__(self, supported_file_types: list, max_pages_sync: int, max_pages_async: int, max_size_bytes_sync: int, max_size_bytes_async: int):\n",
    "        self.supported_file_types = list(supported_file_type.casefold() for supported_file_type in supported_file_types)\n",
    "        self.max_pages_sync = max_pages_sync\n",
    "        self.max_pages_async = max_pages_async\n",
    "        self.max_size_bytes_sync = max_size_bytes_sync\n",
    "        self.max_size_bytes_async = max_size_bytes_async\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = (\n",
    "            f\"supported_file_types: {self.supported_file_types}\\n\"\n",
    "            f\"max_pages_sync: {self.max_pages_sync}\\n\"\n",
    "            f\"max_pages_async: {self.max_pages_async}\\n\"\n",
    "            f\"max_size_bytes_sync: {humanize.naturalsize(self.max_size_bytes_sync, binary=True)}\\n\"\n",
    "            f\"max_size_bytes_async: {humanize.naturalsize(self.max_size_bytes_async, binary=True)}\\n\"\n",
    "        )\n",
    "        return string\n",
    "\n",
    "class ProcessorDetails():\n",
    "    def __init__(self, processor_id: str, processor_type: documentai.ProcessorType, processor_limits: ProcessorLimits):\n",
    "        self.processor_id = processor_id\n",
    "        self.processor_type = processor_type\n",
    "        self.processor_limits = processor_limits\n",
    "    \n",
    "    def __repr__(self):\n",
    "        string = (\n",
    "            f\"processor_id: {self.processor_id}\\n\"\n",
    "            f\"processor_type: {{\\n{indent(str(self.processor_type),'  ',lambda line: True)}}}\\n\"\n",
    "            f\"processor_limits: {{\\n{indent(str(self.processor_limits),'  ',lambda line: True)}}}\"\n",
    "        )\n",
    "        return string\n",
    "\n",
    "def get_processor_details(project_id: str, location: str, processor_id: str):\n",
    "    processor = get_processor(project_id = project_id, location = location, processor_id = processor_id)\n",
    "    processor_types = fetch_processor_types(project_id = project_id, location = location)\n",
    "    processor_type = next((processor_type for processor_type in processor_types if processor.type_ == processor_type.type_), None)\n",
    "    with open('../processor_limits.json') as processor_limits_file:\n",
    "         all_processor_limits = json.load(processor_limits_file)\n",
    "    processor_limits = next((processor_limits for processor_limits in all_processor_limits if processor.type_ == processor_limits[\"type_\"]), None)\n",
    "    if processor_limits:\n",
    "        processor_limits = ProcessorLimits(supported_file_types = processor_limits[\"supported_file_types\"], max_pages_sync = processor_limits[\"max_pages_sync\"], max_pages_async = processor_limits[\"max_pages_async\"], max_size_bytes_sync = processor_limits[\"max_size_bytes_sync\"], max_size_bytes_async = processor_limits[\"max_size_bytes_async\"])\n",
    "    processor_details = ProcessorDetails(\n",
    "        processor_id = processor_id,\n",
    "        processor_type = processor_type,\n",
    "        processor_limits = processor_limits\n",
    "    )\n",
    "    return processor_details\n",
    "\n",
    "def process(project_id: str, location: str, processor_id: str, content: bytes, mime_type: str, skip_human_review: bool = False) -> dict:\n",
    "    \"\"\"Synchronous (online) process document using REST API.\n",
    "\n",
    "    Processes document content with given mime type and blocks until result is returned.\n",
    "    Optionally allows to skip human review if enabled for the processor.\n",
    "    See details at\n",
    "    https://cloud.google.com/document-ai/docs/reference/rest/v1/projects.locations.processors/process\n",
    "\n",
    "    Args:\n",
    "        project_id: GCP Project ID (e.g. 940142200552).\n",
    "        location: Location to be used for the request (e.g. eu or us)\n",
    "        processor_id: Processor ID (e.g. 7f9cd174a388594a).\n",
    "        content: Document content as byte string.\n",
    "        mime_type: An IANA MIME type (RFC6838).\n",
    "        skip_human_review: Optional; Whether Human Review feature should be skipped for this request. Default to false.\n",
    "\n",
    "    Returns:\n",
    "        A dict containing processed document and human_review_status.\n",
    "        See details at\n",
    "        https://cloud.google.com/document-ai/docs/reference/rest/v1/ProcessResponse\n",
    "    \"\"\"\n",
    "    \n",
    "    client = get_client(location = location)\n",
    "\n",
    "    processor = get_processor(project_id = project_id, location = location, processor_id = processor_id)\n",
    "    if not processor:\n",
    "        raise Exception(f\"Processor with ID {processor_id} not found\")\n",
    "\n",
    "    processor_details = get_processor_details(project_id = project_id, location = location, processor_id = processor_id)\n",
    "\n",
    "    # check if request is within supported limits\n",
    "    if processor_details.processor_limits:\n",
    "        content_size = len(content)\n",
    "        if content_size > processor_details.processor_limits.max_size_bytes_sync:\n",
    "            raise Exception(f\"Content of size {humanize.naturalsize(content_size, binary=True)} is larger than the {humanize.naturalsize(processor_details.processor_limits.max_size_bytes_sync, binary=True)} limit of synchronous processing, please use batch processing.\")\n",
    "\n",
    "        page_count = 1\n",
    "        \n",
    "        if mimetypes.guess_extension(mime_type)[1:] not in processor_details.processor_limits.supported_file_types:\n",
    "            raise Exception(f\"MIME type {mime_type} not supported by processor\")\n",
    "\n",
    "        if mime_type == \"image/tiff\":\n",
    "            page_count = Image.open(io.BytesIO(content)).n_frames\n",
    "\n",
    "        if mime_type == \"application/pdf\":\n",
    "            parser = PDFParser(io.BytesIO(content))\n",
    "            document = PDFDocument(parser)\n",
    "\n",
    "            # This will give you the count of pages\n",
    "            page_count = resolve1(document.catalog['Pages'])['Count']\n",
    "\n",
    "        if page_count > processor_details.processor_limits.max_pages_sync:\n",
    "            raise Exception(f\"Page count of {page_count} is larger than {processor_details.processor_limits.max_pages_sync} page limit of synchronous processing, please use batch processing.\")\n",
    "    else:\n",
    "        raise Warining(f\"Processor details for processor with ID {processor_id} not found\")\n",
    "\n",
    "    # Create raw document from image content\n",
    "    raw_document = documentai.RawDocument(\n",
    "        content = content,\n",
    "        mime_type = mime_type\n",
    "    )\n",
    "\n",
    "    # Process document\n",
    "    process_request = documentai.ProcessRequest(\n",
    "        name = processor.name,\n",
    "        raw_document = raw_document,\n",
    "        skip_human_review = skip_human_review\n",
    "    )\n",
    "\n",
    "    return client.process_document(request=process_request)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b15bfb",
   "metadata": {},
   "source": [
    "#### Process documents in a GCS Bucket with Prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b24fc8",
   "metadata": {},
   "source": [
    "Documents in a GCS Bucket can be processed using `process_gcs_bucket`. All documents in the bucket (optionally filtered by `prefix`) will be processed if they have a file type supported by the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b72e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s process_gcs_uri,process_gcs_bucket ../docai\n",
    "def process_gcs_uri(project_id: str, location: str, processor_id: str, gcs_uri: str, skip_human_review: bool = False):\n",
    "    # Instantiate a Google Cloud Storage Client\n",
    "    storage_client = storage.Client()\n",
    "    if not gcs_uri.startswith(\"gs://\"):\n",
    "        raise Exception(f\"gcs_uri {gcs_uri} missing gs:// prefix.\")\n",
    "    \n",
    "    mime_type = mimetypes.guess_type(gcs_uri)[0]\n",
    "    if not mime_type:\n",
    "        raise Exception(f\"MIME type of gcs_uri {gcs_uri} could not be guessed from file extension.\")\n",
    "    processor_details = get_processor_details(project_id = project_id, location = location, processor_id = processor_id)\n",
    "    if processor_details.processor_limits:\n",
    "        if mimetypes.guess_extension(mime_type)[1:].casefold() not in processor_details.processor_limits.supported_file_types:\n",
    "            raise Exception(f\"MIME type {mime_type} of {gcs_uri} not supported by processor\")\n",
    "    \n",
    "    blob = storage.Blob.from_string(uri = gcs_uri, client = storage_client)\n",
    "    image_content = blob.download_as_string()    \n",
    "    \n",
    "    result = process(project_id = project_id, location = location, processor_id = processor_id, content = image_content, mime_type = mime_type)\n",
    "    result.document.uri = gcs_uri\n",
    "    return result\n",
    "\n",
    "def process_gcs_bucket(project_id: str, location: str, processor_id: str, bucket: str, prefix: str = \"\", skip_human_review: bool = False) -> list:\n",
    "    # Instantiate a Google Cloud Storage Client\n",
    "    storage_client = storage.Client()\n",
    "    blobs = storage_client.list_blobs(bucket, prefix = prefix)\n",
    "    results = []\n",
    "    pbar = tqdm(list(blobs),unit = \"document\")\n",
    "    for blob in pbar:\n",
    "        if not blob.name.endswith('/'):\n",
    "            gcs_uri = f\"gs://{bucket}/{blob.name}\"            \n",
    "            pbar.set_postfix({\"document\": gcs_uri})\n",
    "            pbar.refresh()\n",
    "            try:\n",
    "                results.append(process_gcs_uri(project_id = project_id, location = location, processor_id = processor_id, gcs_uri = gcs_uri, skip_human_review = skip_human_review))\n",
    "            except Exception as e: \n",
    "                tqdm.write(\"\\x1b[31m\" + str(e) + \"\\x1b[0m\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e008b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = process_gcs_bucket(project_id = PROJECT_ID, location = LOCATION, processor_id = PROCESSOR_ID, bucket = GCS_INPUT_BUCKET, prefix = GCS_INPUT_PREFIX)\n",
    "\n",
    "output = []\n",
    "for response in responses:\n",
    "    pages = response.document.pages\n",
    "    for page in pages:\n",
    "        output.append((f\"{response.document.uri} - Page {page.page_number}/{len(pages)}\",page))     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135e91e2",
   "metadata": {},
   "source": [
    "The results can be interactively explored with ipywidgets using `display_ocr_output()`. Toggling the BLOCK, PARA, LINE and TOKEN buttons will show or hide the corresponding bounding boxes. Note: For large documents rendering the result may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738dc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s FeatureType,get_page_bounds,render_ocr_page,display_ocr_output ../docai\n",
    "class FeatureType(Enum):\n",
    "    PAGE = 1\n",
    "    BLOCK = 2\n",
    "    PARA = 3\n",
    "    LINE = 4\n",
    "    TOKEN = 5\n",
    "\n",
    "def get_page_bounds(page, feature):\n",
    "    \"\"\"Returns document bounds given the OCR output page.\"\"\"\n",
    "\n",
    "    bounds = []\n",
    "\n",
    "    # Collect specified feature bounds by enumerating all document features\n",
    "    if (feature == FeatureType.BLOCK):\n",
    "        for block in page.blocks:\n",
    "            if not block.layout.bounding_poly.vertices:\n",
    "                block.layout.bounding_poly.vertices = []\n",
    "                for normalized_vertice in block.layout.bounding_poly.normalized_vertices:\n",
    "                    block.layout.bounding_poly.vertices.append(documentai.Vertex(x=int(normalized_vertice.x * page.image.width),y=int(normalized_vertice.y * page.image.height)))\n",
    "            bounds.append(block.layout.bounding_poly)\n",
    "    if (feature == FeatureType.PARA):\n",
    "        for paragraph in page.paragraphs:\n",
    "            if not paragraph.layout.bounding_poly.vertices:\n",
    "                paragraph.layout.bounding_poly.vertices = []\n",
    "                for normalized_vertice in paragraph.layout.bounding_poly.normalized_vertices:\n",
    "                    paragraph.layout.bounding_poly.vertices.append(documentai.Vertex(x=int(normalized_vertice.x * page.image.width),y=int(normalized_vertice.y * page.image.height)))\n",
    "            bounds.append(paragraph.layout.bounding_poly)\n",
    "    if (feature == FeatureType.LINE):        \n",
    "        for line in page.lines:\n",
    "            if not line.layout.bounding_poly.vertices:\n",
    "                line.layout.bounding_poly.vertices = []\n",
    "                for normalized_vertice in line.layout.bounding_poly.normalized_vertices:\n",
    "                    line.layout.bounding_poly.vertices.append(documentai.Vertex(x=int(normalized_vertice.x * page.image.width),y=int(normalized_vertice.y * page.image.height)))\n",
    "            bounds.append(line.layout.bounding_poly)\n",
    "    if (feature == FeatureType.TOKEN):        \n",
    "        for token in page.tokens:\n",
    "            if not token.layout.bounding_poly.vertices:\n",
    "                token.layout.bounding_poly.vertices = []\n",
    "                for normalized_vertice in token.layout.bounding_poly.normalized_vertices:\n",
    "                    token.layout.bounding_poly.vertices.append(documentai.Vertex(x=int(normalized_vertice.x * page.image.width),y=int(normalized_vertice.y * page.image.height)))\n",
    "            bounds.append(token.layout.bounding_poly)\n",
    "\n",
    "\n",
    "    # The list `bounds` contains the coordinates of the bounding boxes.\n",
    "    return bounds\n",
    "\n",
    "def render_ocr_page(page, block = True, para = True, line = True, token = True):  \n",
    "    image = Image.open(io.BytesIO(page.image.content))\n",
    "\n",
    "    # this will draw the bounding boxes for block, paragraph, line and token\n",
    "    if block:\n",
    "        bounds = get_page_bounds(page, FeatureType.BLOCK)\n",
    "        draw_boxes(image, bounds, color='blue', width=8)\n",
    "    if para:\n",
    "        bounds = get_page_bounds(page, FeatureType.PARA)\n",
    "        draw_boxes(image, bounds, color='red',width=6)\n",
    "    if line:\n",
    "        bounds = get_page_bounds(page, FeatureType.LINE)\n",
    "        draw_boxes(image, bounds, color='yellow',width=4)\n",
    "    if token:\n",
    "        bounds = get_page_bounds(page, FeatureType.TOKEN)\n",
    "        draw_boxes(image, bounds, color='green',width=2)\n",
    "\n",
    "    image.show()\n",
    "\n",
    "    # uncomment if you want to save the image with bounding boxes locally\n",
    "    #image.save(document.name)\n",
    "\n",
    "def display_ocr_output(output: list):\n",
    "    dropdown = widgets.Dropdown(options=output)\n",
    "    block = widgets.ToggleButton(description='BLOCK', value=True)\n",
    "    para = widgets.ToggleButton(description='PARA', value=False)\n",
    "    line = widgets.ToggleButton(description='LINE', value=False)\n",
    "    token = widgets.ToggleButton(description='TOKEN', value=True)\n",
    "\n",
    "    ui = widgets.HBox([dropdown, block, para, line, token])\n",
    "\n",
    "    def show_page(page, block,para,line,token):\n",
    "        render_ocr_page(page,block,para,line,token)\n",
    "\n",
    "    out = widgets.interactive_output(show_page, {'page':dropdown, 'block':block,'para':para,'line':line,'token':token})\n",
    "\n",
    "    display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effad0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_ocr_output(output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de784fcd",
   "metadata": {},
   "source": [
    "#### Process documents in a local folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af87e39",
   "metadata": {},
   "source": [
    "Documents in a local directory can be processed using `process_dir`. All documents in the directory and its subdirectories will be processed if they have a file type supported by the processor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e286ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s process_file,process_dir ../docai\n",
    "def process_file(project_id: str, location: str, processor_id: str, path: str, skip_human_review: bool = False):\n",
    "    path = Path(path)\n",
    "    if not path.is_file():\n",
    "        raise Exception(f\"Path {path} is not a file\")\n",
    "    \n",
    "    mime_type = mimetypes.guess_type(path.name)[0]\n",
    "    processor_details = get_processor_details(project_id = project_id, location = location, processor_id = processor_id)\n",
    "    if path.suffix[1:].casefold() not in (supported_file_type.casefold() for supported_file_type in processor_details.processor_limits.supported_file_types):\n",
    "        raise Exception(f\"MIME type {mime_type} of {gcs_uri} not supported by processor\")\n",
    "    image_content = open(path, \"rb\").read()\n",
    "    result = process(project_id = project_id, location = location, processor_id = processor_id, content = image_content, mime_type = mime_type, skip_human_review = skip_human_review)\n",
    "    result.document.uri = path.resolve().as_uri()\n",
    "    return result\n",
    "\n",
    "def process_dir(project_id: str, location: str, processor_id: str, path: str, skip_human_review: bool = False):\n",
    "    path = Path(path)\n",
    "    results = []\n",
    "    if not path.exists():\n",
    "        raise(f\"Directory {path} does not exist\")\n",
    "    if not path.is_dir():\n",
    "        raise Exception(f\"{path} is not a directory\")\n",
    "        \n",
    "    files = path.rglob('*')\n",
    "    pbar = tqdm(list(files), unit = \"document\")\n",
    "    for path in pbar:\n",
    "        pbar.set_postfix({\"document\": path.resolve().as_uri()})\n",
    "        pbar.refresh()\n",
    "        try:\n",
    "            results.append(process_file(project_id = project_id, location = location, processor_id = processor_id, path = path, skip_human_review = skip_human_review))\n",
    "        except Exception as e: \n",
    "            pbar.write(\"\\x1b[31m\" + str(e) + \"\\x1b[0m\")  \n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = process_dir(project_id = PROJECT_ID, location = LOCATION, processor_id = PROCESSOR_ID, path = LOCAL_INPUT_PATH)\n",
    "\n",
    "output = []\n",
    "for response in responses:\n",
    "    pages = response.document.pages\n",
    "    for page in pages:\n",
    "        output.append((f\"{response.document.uri} - Page {page.page_number}/{len(pages)}\",page))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7978170",
   "metadata": {},
   "source": [
    "The results can be interactively explored with ipywidgets using `display_ocr_output()`. Toggling the BLOCK, PARA, LINE and TOKEN buttons will show or hide the corresponding bounding boxes. Note: For large documents rendering the result may take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a526001",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_ocr_output(output = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8547fb",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4864cd4",
   "metadata": {},
   "source": [
    "Finally the OCR Processor can be deleted with `delete_processor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d8637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s delete_processor ../docai\n",
    "def delete_processor(project_id: str, location: str, processor_id: str):\n",
    "    client = get_client(location = location)\n",
    "    name = f\"projects/{project_id}/locations/{location}/processors/{processor_id}\"\n",
    "    result = client.delete_processor(name = name)\n",
    "    print(f\"Processor {name} successfully deleted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681e2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_processor(\n",
    "    project_id = PROJECT_ID,\n",
    "    location = LOCATION,\n",
    "    processor_id = PROCESSOR_ID\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m76"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
